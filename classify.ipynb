{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "11d7be56-dae4-4a40-b2f0-3a6717ca4d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Tianshuo Zhou\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "cudnn.benchmark = True\n",
    "# plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab226f4-d57c-4550-9395-9c3380626106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import  train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2957af65-1d87-4d61-a027-e5edd143b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import SvdDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abcd2865-16e2-4447-bf22-cdefaadfd740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triples: \n",
      "[['data/flowers/daisy/100080576_f52e8ee070_n.jpg' '0']\n",
      " ['data/flowers/daisy/10140303196_b88d3d6cec.jpg' '0']\n",
      " ['data/flowers/daisy/10172379554_b296050f82_n.jpg' '0']\n",
      " ['data/flowers/daisy/10172567486_2748826a8b.jpg' '0']\n",
      " ['data/flowers/daisy/10172636503_21bededa75_n.jpg' '0']\n",
      " ['data/flowers/daisy/102841525_bd6628ae3c.jpg' '0']\n",
      " ['data/flowers/daisy/10300722094_28fa978807_n.jpg' '0']\n",
      " ['data/flowers/daisy/1031799732_e7f4008c03.jpg' '0']\n",
      " ['data/flowers/daisy/10391248763_1d16681106_n.jpg' '0']\n",
      " ['data/flowers/daisy/10437754174_22ec990b77_m.jpg' '0']]\n",
      "[['data/flowers/tulip/9048307967_40a164a459_m.jpg' '4']\n",
      " ['data/flowers/tulip/924782410_94ed7913ca_m.jpg' '4']\n",
      " ['data/flowers/tulip/9378657435_89fabf13c9_n.jpg' '4']\n",
      " ['data/flowers/tulip/9444202147_405290415b_n.jpg' '4']\n",
      " ['data/flowers/tulip/9446982168_06c4d71da3_n.jpg' '4']\n",
      " ['data/flowers/tulip/9831362123_5aac525a99_n.jpg' '4']\n",
      " ['data/flowers/tulip/9870557734_88eb3b9e3b_n.jpg' '4']\n",
      " ['data/flowers/tulip/9947374414_fdf1d0861c_n.jpg' '4']\n",
      " ['data/flowers/tulip/9947385346_3a8cacea02_n.jpg' '4']\n",
      " ['data/flowers/tulip/9976515506_d496c5e72c.jpg' '4']]\n"
     ]
    }
   ],
   "source": [
    "path_data = 'data/flowers.npy'\n",
    "batch_size = 256\n",
    "workers = 16\n",
    "image_size = 224\n",
    "\n",
    "triples = np.load(path_data)\n",
    "print('triples: ')\n",
    "print(triples[:10])\n",
    "print(triples[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a214e2-34eb-41ad-a39a-4afea40d3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_train, triples_test = train_test_split(triples, test_size=3000, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871dd9f5-0c15-4807-8fd8-cfbe5bdb366d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1317, 3000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples_train), len(triples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f19de5dc-deda-490e-9ac3-d81aaf927305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataloader_train): 6, len(dataloader_train): 6\n",
      "0 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "1 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "2 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "3 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "4 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "5 torch.Size([37, 3, 224, 224]) torch.Size([37])\n",
      "len(dataset_test): 3000, len(dataloader_test): 12\n",
      "0 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "1 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "2 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "3 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "4 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "5 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "6 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "7 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "8 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "9 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "10 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "11 torch.Size([184, 3, 224, 224]) torch.Size([184])\n"
     ]
    }
   ],
   "source": [
    "transform_compose=transforms.Compose([\n",
    "                           transforms.Resize(image_size),\n",
    "                           transforms.CenterCrop(image_size),\n",
    "                           # transforms.RandomHorizontalFlip(),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                       ])\n",
    "\n",
    "dataset_train = SvdDataset(files_list=triples_train[:,0], labels_list=triples_train[:, 1], transform=transform_compose, files_list_svd=triples_train[:, 0])\n",
    "# Create the dataloader\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "print(f'len(dataloader_train): {len(dataloader_train)}, len(dataloader_train): {len(dataloader_train)}')\n",
    "for i, batch in enumerate(dataloader_train):\n",
    "    images, labels = batch\n",
    "    print(i, images.shape, labels.shape)\n",
    "\n",
    "dataset_test = SvdDataset(files_list=triples_test[:,0], labels_list=triples_test[:, 1], transform=transform_compose, files_list_svd=triples_test[:, 0])\n",
    "# Create the dataloader\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "print(f'len(dataset_test): {len(dataset_test)}, len(dataloader_test): {len(dataloader_test)}')\n",
    "\n",
    "for i, batch in enumerate(dataloader_test):\n",
    "    images, labels = batch\n",
    "    print(i, images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19aee112-f003-44e6-b87a-978b65c49e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c18dca4-75c7-4502-9a7a-80727b9c01e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aef964f-1616-468f-bd88-e035a70f3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=25, scheduler=None):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            # with torch.set_grad_enabled(phase == 'train'):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "        print(f'epoch: {epoch: 2d}, loss: {epoch_loss:.4f}, acc: {epoch_acc:.4f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9aeba1-ae42-4271-8378-ac1e60b63722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    running_corrects = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "        print(f'Valid Acc: {epoch_acc:.4f}')\n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bef32097-bb17-4335-8842-9cd5df1c99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(dataset_train.class_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad036efa-fdc0-4bdc-90ac-a05dbe0c7b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ead27f2-62a5-4f6e-a77c-6704993ff476",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_compose=transforms.Compose([\n",
    "                           transforms.Resize(image_size),\n",
    "                           transforms.CenterCrop(image_size),\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                       ])\n",
    "\n",
    "transform_compose_test=transforms.Compose([\n",
    "                           transforms.Resize(image_size),\n",
    "                           transforms.CenterCrop(image_size),\n",
    "                           # transforms.RandomHorizontalFlip(),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ff37254-7228-4bec-9086-ee0056d2c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataloader_train): 6, len(dataloader_train): 6\n",
      "0 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "1 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "2 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "3 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "4 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "5 torch.Size([37, 3, 224, 224]) torch.Size([37])\n",
      "len(dataset_test): 3000, len(dataloader_test): 12\n",
      "0 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "1 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "2 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "3 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "4 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "5 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "6 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "7 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "8 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "9 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "10 torch.Size([256, 3, 224, 224]) torch.Size([256])\n",
      "11 torch.Size([184, 3, 224, 224]) torch.Size([184])\n"
     ]
    }
   ],
   "source": [
    "dataset_train = SvdDataset(files_list=triples_train[:,0], labels_list=triples_train[:, 1], transform=transform_compose, files_list_svd=triples_train[:, 0])\n",
    "# Create the dataloader\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "print(f'len(dataloader_train): {len(dataloader_train)}, len(dataloader_train): {len(dataloader_train)}')\n",
    "for i, batch in enumerate(dataloader_train):\n",
    "    images, labels = batch\n",
    "    print(i, images.shape, labels.shape)\n",
    "\n",
    "dataset_test = SvdDataset(files_list=triples_test[:,0], labels_list=triples_test[:, 1], transform=transform_compose_test, files_list_svd=triples_test[:, 0])\n",
    "# Create the dataloader\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "print(f'len(dataset_test): {len(dataset_test)}, len(dataloader_test): {len(dataloader_test)}')\n",
    "\n",
    "for i, batch in enumerate(dataloader_test):\n",
    "    images, labels = batch\n",
    "    print(i, images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a5cbed2-ffa4-4cbf-8549-d3efe0df9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_svd(tranfm_train, train_size=2000, train_epochs=20):\n",
    "    test_size = len(triples) - train_size\n",
    "    triples_train, triples_test = train_test_split(triples, test_size=test_size, random_state=2)\n",
    "    files_svd_train = [file.replace('flowers', 'flowers_svd40') for file in triples_train[:, 0]]\n",
    "    print(f'files_svd_train: {files_svd_train[:2]}')\n",
    "    torch.manual_seed(2)\n",
    "    dataset_train = SvdDataset(files_list=triples_train[:,0], labels_list=triples_train[:, 1], transform=tranfm_train, files_list_svd=files_svd_train)\n",
    "    # Create the dataloader\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "    print(f'len(dataloader_train): {len(dataset_train)}, len(dataloader_train): {len(dataloader_train)}')\n",
    "    # for i, batch in enumerate(dataloader_train):\n",
    "    #     images, labels = batch\n",
    "    #     print(i, images.shape, labels.shape)\n",
    "\n",
    "    dataset_test = SvdDataset(files_list=triples_test[:,0], labels_list=triples_test[:, 1], transform=transform_compose_test, files_list_svd=triples_test[:, 0])\n",
    "    # Create the dataloader\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "    print(f'len(dataset_test): {len(dataset_test)}, len(dataloader_test): {len(dataloader_test)}')\n",
    "\n",
    "#     for i, batch in enumerate(dataloader_test):\n",
    "#         images, labels = batch\n",
    "#         print(i, images.shape, labels.shape)\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    # Here the size of each output sample is set to 2.\n",
    "    # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_class)\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "    model_train = train_model(model_ft, dataloader_train, criterion, optimizer_ft, num_epochs=train_epochs, scheduler=exp_lr_scheduler)\n",
    "    acc_eval = eval_model(model_train, dataloader_test)\n",
    "    print(f'acc_val: {acc_eval: .4f}')\n",
    "    return acc_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1cbc6339-3824-47da-9d9a-e02274a3a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(tranfm_train, train_size=2000, train_epochs=20):\n",
    "    test_size = len(triples) - train_size\n",
    "    triples_train, triples_test = train_test_split(triples, test_size=test_size, random_state=2)\n",
    "    dataset_train = SvdDataset(files_list=triples_train[:,0], labels_list=triples_train[:, 1], transform=tranfm_train, files_list_svd=triples_train[:, 0])\n",
    "    # Create the dataloader\n",
    "    torch.manual_seed(2)\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "    print(f'len(dataloader_train): {len(dataset_train)}, len(dataloader_train): {len(dataloader_train)}')\n",
    "    # for i, batch in enumerate(dataloader_train):\n",
    "    #     images, labels = batch\n",
    "    #     print(i, images.shape, labels.shape)\n",
    "\n",
    "    dataset_test = SvdDataset(files_list=triples_test[:,0], labels_list=triples_test[:, 1], transform=transform_compose_test, files_list_svd=triples_test[:, 0])\n",
    "    # Create the dataloader\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "    print(f'len(dataset_test): {len(dataset_test)}, len(dataloader_test): {len(dataloader_test)}')\n",
    "\n",
    "#     for i, batch in enumerate(dataloader_test):\n",
    "#         images, labels = batch\n",
    "#         print(i, images.shape, labels.shape)\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    # Here the size of each output sample is set to 2.\n",
    "    # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_class)\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "    model_train = train_model(model_ft, dataloader_train, criterion, optimizer_ft, num_epochs=train_epochs, scheduler=exp_lr_scheduler)\n",
    "    acc_eval = eval_model(model_train, dataloader_test)\n",
    "    print(f'acc_val: {acc_eval: .4f}')\n",
    "    return acc_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57bdddf6-74a6-4f70-93f2-a3ab23806eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "transform_compose=transforms.Compose([\n",
    "                           transforms.Resize(image_size),\n",
    "                           transforms.CenterCrop(image_size),\n",
    "                           # transforms.RandomHorizontalFlip(),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6a94d74-c27c-499d-b6e3-10bc71dba6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataloader_train): 500, len(dataloader_train): 4\n",
      "len(dataset_test): 3817, len(dataloader_test): 30\n",
      "epoch:  0, loss: 1.7475, acc: 0.2120\n",
      "epoch:  1, loss: 1.1253, acc: 0.6000\n",
      "epoch:  2, loss: 0.6073, acc: 0.8640\n",
      "epoch:  3, loss: 0.3683, acc: 0.9140\n",
      "epoch:  4, loss: 0.2403, acc: 0.9440\n",
      "epoch:  5, loss: 0.1454, acc: 0.9800\n",
      "epoch:  6, loss: 0.0879, acc: 0.9880\n",
      "epoch:  7, loss: 0.0628, acc: 0.9980\n",
      "epoch:  8, loss: 0.0406, acc: 0.9980\n",
      "epoch:  9, loss: 0.0236, acc: 0.9980\n",
      "Valid Acc: 0.8779\n",
      "acc_val:  0.8779\n",
      "files_svd_train: ['data/flowers_svd40/dandelion/674407101_57676c40fb.jpg', 'data/flowers_svd40/tulip/16570062929_6c41c66387_n.jpg']\n",
      "len(dataloader_train): 500, len(dataloader_train): 4\n",
      "len(dataset_test): 3817, len(dataloader_test): 30\n",
      "epoch:  0, loss: 1.7679, acc: 0.2080\n",
      "epoch:  1, loss: 1.1827, acc: 0.5620\n",
      "epoch:  2, loss: 0.6849, acc: 0.8280\n",
      "epoch:  3, loss: 0.4400, acc: 0.8840\n",
      "epoch:  4, loss: 0.2946, acc: 0.9340\n",
      "epoch:  5, loss: 0.1956, acc: 0.9440\n",
      "epoch:  6, loss: 0.1230, acc: 0.9800\n",
      "epoch:  7, loss: 0.0926, acc: 0.9840\n",
      "epoch:  8, loss: 0.0630, acc: 0.9920\n",
      "epoch:  9, loss: 0.0364, acc: 1.0000\n",
      "Valid Acc: 0.8800\n",
      "acc_val:  0.8800\n",
      "len(dataloader_train): 1000, len(dataloader_train): 8\n",
      "len(dataset_test): 3317, len(dataloader_test): 26\n",
      "epoch:  0, loss: 1.4500, acc: 0.3900\n",
      "epoch:  1, loss: 0.5523, acc: 0.8520\n",
      "epoch:  2, loss: 0.2716, acc: 0.9180\n",
      "epoch:  3, loss: 0.1398, acc: 0.9700\n",
      "epoch:  4, loss: 0.0880, acc: 0.9840\n",
      "epoch:  5, loss: 0.0501, acc: 0.9930\n",
      "epoch:  6, loss: 0.0314, acc: 0.9980\n",
      "epoch:  7, loss: 0.0256, acc: 0.9980\n",
      "epoch:  8, loss: 0.0161, acc: 0.9980\n",
      "epoch:  9, loss: 0.0138, acc: 0.9980\n",
      "Valid Acc: 0.8939\n",
      "acc_val:  0.8939\n",
      "files_svd_train: ['data/flowers_svd40/dandelion/2467980325_237b14c737_m.jpg', 'data/flowers_svd40/dandelion/3005677730_2662753d3f_m.jpg']\n",
      "len(dataloader_train): 1000, len(dataloader_train): 8\n",
      "len(dataset_test): 3317, len(dataloader_test): 26\n",
      "epoch:  0, loss: 1.4769, acc: 0.3890\n",
      "epoch:  1, loss: 0.6067, acc: 0.8280\n",
      "epoch:  2, loss: 0.3173, acc: 0.9040\n",
      "epoch:  3, loss: 0.1854, acc: 0.9430\n",
      "epoch:  4, loss: 0.1192, acc: 0.9740\n",
      "epoch:  5, loss: 0.0782, acc: 0.9890\n",
      "epoch:  6, loss: 0.0517, acc: 0.9910\n",
      "epoch:  7, loss: 0.0376, acc: 0.9980\n",
      "epoch:  8, loss: 0.0269, acc: 0.9980\n",
      "epoch:  9, loss: 0.0185, acc: 1.0000\n",
      "Valid Acc: 0.9011\n",
      "acc_val:  0.9011\n",
      "len(dataloader_train): 1500, len(dataloader_train): 12\n",
      "len(dataset_test): 2817, len(dataloader_test): 23\n",
      "epoch:  0, loss: 1.2215, acc: 0.5180\n",
      "epoch:  1, loss: 0.3775, acc: 0.8813\n",
      "epoch:  2, loss: 0.1837, acc: 0.9467\n",
      "epoch:  3, loss: 0.0967, acc: 0.9753\n",
      "epoch:  4, loss: 0.0478, acc: 0.9947\n",
      "epoch:  5, loss: 0.0293, acc: 0.9980\n",
      "epoch:  6, loss: 0.0186, acc: 0.9993\n",
      "epoch:  7, loss: 0.0149, acc: 0.9993\n",
      "epoch:  8, loss: 0.0119, acc: 0.9993\n",
      "epoch:  9, loss: 0.0102, acc: 0.9993\n",
      "Valid Acc: 0.9049\n",
      "acc_val:  0.9049\n",
      "files_svd_train: ['data/flowers_svd40/daisy/15100730728_a450c5f422_n.jpg', 'data/flowers_svd40/dandelion/2449852402_45d12b9875_n.jpg']\n",
      "len(dataloader_train): 1500, len(dataloader_train): 12\n",
      "len(dataset_test): 2817, len(dataloader_test): 23\n",
      "epoch:  0, loss: 1.2556, acc: 0.5000\n",
      "epoch:  1, loss: 0.4316, acc: 0.8693\n",
      "epoch:  2, loss: 0.2397, acc: 0.9220\n",
      "epoch:  3, loss: 0.1379, acc: 0.9600\n",
      "epoch:  4, loss: 0.0786, acc: 0.9833\n",
      "epoch:  5, loss: 0.0514, acc: 0.9907\n",
      "epoch:  6, loss: 0.0322, acc: 0.9967\n",
      "epoch:  7, loss: 0.0273, acc: 0.9967\n",
      "epoch:  8, loss: 0.0178, acc: 0.9993\n",
      "epoch:  9, loss: 0.0150, acc: 0.9993\n",
      "Valid Acc: 0.9134\n",
      "acc_val:  0.9134\n",
      "len(dataloader_train): 2000, len(dataloader_train): 16\n",
      "len(dataset_test): 2317, len(dataloader_test): 19\n",
      "epoch:  0, loss: 1.0446, acc: 0.6035\n",
      "epoch:  1, loss: 0.2872, acc: 0.9095\n",
      "epoch:  2, loss: 0.1441, acc: 0.9570\n",
      "epoch:  3, loss: 0.0697, acc: 0.9875\n",
      "epoch:  4, loss: 0.0413, acc: 0.9955\n",
      "epoch:  5, loss: 0.0258, acc: 0.9980\n",
      "epoch:  6, loss: 0.0168, acc: 0.9985\n",
      "epoch:  7, loss: 0.0148, acc: 0.9985\n",
      "epoch:  8, loss: 0.0114, acc: 0.9980\n",
      "epoch:  9, loss: 0.0091, acc: 0.9990\n",
      "Valid Acc: 0.9145\n",
      "acc_val:  0.9145\n",
      "files_svd_train: ['data/flowers_svd40/daisy/14350958832_29bdd3a254.jpg', 'data/flowers_svd40/daisy/2611119198_9d46b94392.jpg']\n",
      "len(dataloader_train): 2000, len(dataloader_train): 16\n",
      "len(dataset_test): 2317, len(dataloader_test): 19\n",
      "epoch:  0, loss: 1.0780, acc: 0.5880\n",
      "epoch:  1, loss: 0.3272, acc: 0.8860\n",
      "epoch:  2, loss: 0.1827, acc: 0.9415\n",
      "epoch:  3, loss: 0.1059, acc: 0.9705\n",
      "epoch:  4, loss: 0.0729, acc: 0.9855\n",
      "epoch:  5, loss: 0.0419, acc: 0.9930\n",
      "epoch:  6, loss: 0.0279, acc: 0.9950\n",
      "epoch:  7, loss: 0.0212, acc: 0.9975\n",
      "epoch:  8, loss: 0.0163, acc: 0.9975\n",
      "epoch:  9, loss: 0.0117, acc: 0.9990\n",
      "Valid Acc: 0.9176\n",
      "acc_val:  0.9176\n",
      "len(dataloader_train): 2500, len(dataloader_train): 20\n",
      "len(dataset_test): 1817, len(dataloader_test): 15\n",
      "epoch:  0, loss: 0.9447, acc: 0.6412\n",
      "epoch:  1, loss: 0.2681, acc: 0.9064\n",
      "epoch:  2, loss: 0.1175, acc: 0.9668\n",
      "epoch:  3, loss: 0.0625, acc: 0.9884\n",
      "epoch:  4, loss: 0.0351, acc: 0.9968\n",
      "epoch:  5, loss: 0.0224, acc: 0.9992\n",
      "epoch:  6, loss: 0.0161, acc: 0.9984\n",
      "epoch:  7, loss: 0.0135, acc: 0.9988\n",
      "epoch:  8, loss: 0.0107, acc: 0.9988\n",
      "epoch:  9, loss: 0.0123, acc: 0.9988\n",
      "Valid Acc: 0.9263\n",
      "acc_val:  0.9263\n",
      "files_svd_train: ['data/flowers_svd40/tulip/20282921225_6033da8cd5_n.jpg', 'data/flowers_svd40/tulip/14255917256_84c23c572b.jpg']\n",
      "len(dataloader_train): 2500, len(dataloader_train): 20\n",
      "len(dataset_test): 1817, len(dataloader_test): 15\n",
      "epoch:  0, loss: 0.9826, acc: 0.6228\n",
      "epoch:  1, loss: 0.3076, acc: 0.8972\n",
      "epoch:  2, loss: 0.1582, acc: 0.9512\n",
      "epoch:  3, loss: 0.0982, acc: 0.9748\n",
      "epoch:  4, loss: 0.0574, acc: 0.9868\n",
      "epoch:  5, loss: 0.0381, acc: 0.9944\n",
      "epoch:  6, loss: 0.0237, acc: 0.9968\n",
      "epoch:  7, loss: 0.0189, acc: 0.9980\n",
      "epoch:  8, loss: 0.0167, acc: 0.9976\n",
      "epoch:  9, loss: 0.0142, acc: 0.9980\n",
      "Valid Acc: 0.9268\n",
      "acc_val:  0.9268\n",
      "len(dataloader_train): 3000, len(dataloader_train): 24\n",
      "len(dataset_test): 1317, len(dataloader_test): 11\n",
      "epoch:  0, loss: 0.8556, acc: 0.6687\n",
      "epoch:  1, loss: 0.2331, acc: 0.9220\n",
      "epoch:  2, loss: 0.1108, acc: 0.9683\n",
      "epoch:  3, loss: 0.0552, acc: 0.9900\n",
      "epoch:  4, loss: 0.0296, acc: 0.9973\n",
      "epoch:  5, loss: 0.0196, acc: 0.9983\n",
      "epoch:  6, loss: 0.0146, acc: 0.9990\n",
      "epoch:  7, loss: 0.0120, acc: 0.9987\n",
      "epoch:  8, loss: 0.0089, acc: 0.9990\n",
      "epoch:  9, loss: 0.0082, acc: 0.9987\n",
      "Valid Acc: 0.9286\n",
      "acc_val:  0.9286\n",
      "files_svd_train: ['data/flowers_svd40/daisy/1285423653_18926dc2c8_n.jpg', 'data/flowers_svd40/sunflower/3466923719_b4b6df7f8b_n.jpg']\n",
      "len(dataloader_train): 3000, len(dataloader_train): 24\n",
      "len(dataset_test): 1317, len(dataloader_test): 11\n",
      "epoch:  0, loss: 0.8858, acc: 0.6590\n",
      "epoch:  1, loss: 0.2704, acc: 0.9067\n",
      "epoch:  2, loss: 0.1515, acc: 0.9553\n",
      "epoch:  3, loss: 0.0878, acc: 0.9757\n",
      "epoch:  4, loss: 0.0524, acc: 0.9890\n",
      "epoch:  5, loss: 0.0315, acc: 0.9940\n",
      "epoch:  6, loss: 0.0237, acc: 0.9963\n",
      "epoch:  7, loss: 0.0202, acc: 0.9970\n",
      "epoch:  8, loss: 0.0135, acc: 0.9987\n",
      "epoch:  9, loss: 0.0103, acc: 0.9993\n",
      "Valid Acc: 0.9355\n",
      "acc_val:  0.9355\n",
      "results:  [tensor(0.8779, device='cuda:1', dtype=torch.float64), tensor(0.8939, device='cuda:1', dtype=torch.float64), tensor(0.9049, device='cuda:1', dtype=torch.float64), tensor(0.9145, device='cuda:1', dtype=torch.float64), tensor(0.9263, device='cuda:1', dtype=torch.float64), tensor(0.9286, device='cuda:1', dtype=torch.float64)]\n",
      "results_svd:  [tensor(0.8800, device='cuda:1', dtype=torch.float64), tensor(0.9011, device='cuda:1', dtype=torch.float64), tensor(0.9134, device='cuda:1', dtype=torch.float64), tensor(0.9176, device='cuda:1', dtype=torch.float64), tensor(0.9268, device='cuda:1', dtype=torch.float64), tensor(0.9355, device='cuda:1', dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "results_svd =[]\n",
    "for size in [500, 1000, 1500, 2000, 2500, 3000]:\n",
    "    acc = train_eval(transform_compose, train_size=size, train_epochs=10)\n",
    "    acc_svd = train_eval_svd(transform_compose, train_size=size, train_epochs=10)\n",
    "    results.append(acc)\n",
    "    results_svd.append(acc_svd)\n",
    "print('results: ', results)\n",
    "print('results_svd: ', results_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "620beb80-5be9-4eff-a424-c56d930247f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataloader_train): 3500, len(dataloader_train): 28\n",
      "len(dataset_test): 817, len(dataloader_test): 7\n",
      "epoch:  0, loss: 0.7690, acc: 0.7194\n",
      "epoch:  1, loss: 0.2225, acc: 0.9234\n",
      "epoch:  2, loss: 0.1035, acc: 0.9726\n",
      "epoch:  3, loss: 0.0551, acc: 0.9897\n",
      "epoch:  4, loss: 0.0320, acc: 0.9954\n",
      "epoch:  5, loss: 0.0170, acc: 0.9983\n",
      "epoch:  6, loss: 0.0121, acc: 0.9994\n",
      "epoch:  7, loss: 0.0105, acc: 0.9994\n",
      "epoch:  8, loss: 0.0093, acc: 0.9980\n",
      "epoch:  9, loss: 0.0082, acc: 0.9989\n",
      "Valid Acc: 0.9461\n",
      "acc_val:  0.9461\n",
      "files_svd_train: ['data/flowers_svd40/tulip/485266837_671def8627.jpg', 'data/flowers_svd40/tulip/13979840624_28466cb3ec_n.jpg']\n",
      "len(dataloader_train): 3500, len(dataloader_train): 28\n",
      "len(dataset_test): 817, len(dataloader_test): 7\n",
      "epoch:  0, loss: 0.8025, acc: 0.7069\n",
      "epoch:  1, loss: 0.2656, acc: 0.9097\n",
      "epoch:  2, loss: 0.1452, acc: 0.9540\n",
      "epoch:  3, loss: 0.0878, acc: 0.9740\n",
      "epoch:  4, loss: 0.0524, acc: 0.9874\n",
      "epoch:  5, loss: 0.0293, acc: 0.9957\n",
      "epoch:  6, loss: 0.0184, acc: 0.9983\n",
      "epoch:  7, loss: 0.0152, acc: 0.9971\n",
      "epoch:  8, loss: 0.0117, acc: 0.9983\n",
      "epoch:  9, loss: 0.0099, acc: 0.9986\n",
      "Valid Acc: 0.9510\n",
      "acc_val:  0.9510\n"
     ]
    }
   ],
   "source": [
    "acc = train_eval(transform_compose, train_size=3500, train_epochs=10)\n",
    "acc_svd = train_eval_svd(transform_compose, train_size=3500, train_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bc64244b-aaaa-447c-b534-98ff426208ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results:  [tensor(0.8779, device='cuda:1', dtype=torch.float64), tensor(0.8939, device='cuda:1', dtype=torch.float64), tensor(0.9049, device='cuda:1', dtype=torch.float64), tensor(0.9145, device='cuda:1', dtype=torch.float64), tensor(0.9263, device='cuda:1', dtype=torch.float64), tensor(0.9286, device='cuda:1', dtype=torch.float64), tensor(0.9461, device='cuda:1', dtype=torch.float64)]\n",
      "results_svd:  [tensor(0.8800, device='cuda:1', dtype=torch.float64), tensor(0.9011, device='cuda:1', dtype=torch.float64), tensor(0.9134, device='cuda:1', dtype=torch.float64), tensor(0.9176, device='cuda:1', dtype=torch.float64), tensor(0.9268, device='cuda:1', dtype=torch.float64), tensor(0.9355, device='cuda:1', dtype=torch.float64), tensor(0.9510, device='cuda:1', dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "results.append(acc)\n",
    "results_svd.append(acc_svd)\n",
    "print('results: ', results)\n",
    "print('results_svd: ', results_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bcc702c1-b86a-415c-9b33-d621e0d87610",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_compose_flip=transforms.Compose([\n",
    "                           transforms.Resize(image_size),\n",
    "                           transforms.CenterCrop(image_size),\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9b04a2b-613b-400e-a2d6-b8dccfefdde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataloader_train): 500, len(dataloader_train): 4\n",
      "len(dataset_test): 3817, len(dataloader_test): 30\n",
      "epoch:  0, loss: 1.7612, acc: 0.2180\n",
      "epoch:  1, loss: 1.1480, acc: 0.5840\n",
      "epoch:  2, loss: 0.6464, acc: 0.8340\n",
      "epoch:  3, loss: 0.4138, acc: 0.8960\n",
      "epoch:  4, loss: 0.2816, acc: 0.9240\n",
      "epoch:  5, loss: 0.1837, acc: 0.9580\n",
      "epoch:  6, loss: 0.1272, acc: 0.9660\n",
      "epoch:  7, loss: 0.0995, acc: 0.9800\n",
      "epoch:  8, loss: 0.0637, acc: 0.9920\n",
      "epoch:  9, loss: 0.0412, acc: 0.9980\n",
      "Valid Acc: 0.8790\n",
      "acc_val:  0.8790\n",
      "files_svd_train: ['data/flowers_svd40/dandelion/674407101_57676c40fb.jpg', 'data/flowers_svd40/tulip/16570062929_6c41c66387_n.jpg']\n",
      "len(dataloader_train): 500, len(dataloader_train): 4\n",
      "len(dataset_test): 3817, len(dataloader_test): 30\n",
      "epoch:  0, loss: 1.7679, acc: 0.2080\n",
      "epoch:  1, loss: 1.1827, acc: 0.5620\n",
      "epoch:  2, loss: 0.6849, acc: 0.8280\n",
      "epoch:  3, loss: 0.4400, acc: 0.8840\n",
      "epoch:  4, loss: 0.2945, acc: 0.9340\n",
      "epoch:  5, loss: 0.1957, acc: 0.9440\n",
      "epoch:  6, loss: 0.1232, acc: 0.9800\n",
      "epoch:  7, loss: 0.0925, acc: 0.9840\n",
      "epoch:  8, loss: 0.0631, acc: 0.9920\n",
      "epoch:  9, loss: 0.0364, acc: 1.0000\n",
      "Valid Acc: 0.8805\n",
      "acc_val:  0.8805\n",
      "len(dataloader_train): 1000, len(dataloader_train): 8\n",
      "len(dataset_test): 3317, len(dataloader_test): 26\n",
      "epoch:  0, loss: 1.4555, acc: 0.3910\n",
      "epoch:  1, loss: 0.5679, acc: 0.8400\n",
      "epoch:  2, loss: 0.2948, acc: 0.9100\n",
      "epoch:  3, loss: 0.1680, acc: 0.9510\n",
      "epoch:  4, loss: 0.1230, acc: 0.9730\n",
      "epoch:  5, loss: 0.0722, acc: 0.9820\n",
      "epoch:  6, loss: 0.0486, acc: 0.9940\n",
      "epoch:  7, loss: 0.0442, acc: 0.9940\n",
      "epoch:  8, loss: 0.0261, acc: 0.9970\n",
      "epoch:  9, loss: 0.0209, acc: 0.9990\n",
      "Valid Acc: 0.8993\n",
      "acc_val:  0.8993\n",
      "files_svd_train: ['data/flowers_svd40/dandelion/2467980325_237b14c737_m.jpg', 'data/flowers_svd40/dandelion/3005677730_2662753d3f_m.jpg']\n",
      "len(dataloader_train): 1000, len(dataloader_train): 8\n",
      "len(dataset_test): 3317, len(dataloader_test): 26\n",
      "epoch:  0, loss: 1.4769, acc: 0.3890\n",
      "epoch:  1, loss: 0.6067, acc: 0.8280\n",
      "epoch:  2, loss: 0.3177, acc: 0.9040\n",
      "epoch:  3, loss: 0.1855, acc: 0.9440\n",
      "epoch:  4, loss: 0.1195, acc: 0.9730\n",
      "epoch:  5, loss: 0.0781, acc: 0.9900\n",
      "epoch:  6, loss: 0.0519, acc: 0.9910\n",
      "epoch:  7, loss: 0.0377, acc: 0.9980\n",
      "epoch:  8, loss: 0.0270, acc: 0.9980\n",
      "epoch:  9, loss: 0.0184, acc: 1.0000\n",
      "Valid Acc: 0.9011\n",
      "acc_val:  0.9011\n",
      "len(dataloader_train): 1500, len(dataloader_train): 12\n",
      "len(dataset_test): 2817, len(dataloader_test): 23\n",
      "epoch:  0, loss: 1.2265, acc: 0.5180\n",
      "epoch:  1, loss: 0.3939, acc: 0.8747\n",
      "epoch:  2, loss: 0.2118, acc: 0.9360\n",
      "epoch:  3, loss: 0.1332, acc: 0.9613\n",
      "epoch:  4, loss: 0.0784, acc: 0.9780\n",
      "epoch:  5, loss: 0.0519, acc: 0.9933\n",
      "epoch:  6, loss: 0.0332, acc: 0.9967\n",
      "epoch:  7, loss: 0.0288, acc: 0.9973\n",
      "epoch:  8, loss: 0.0179, acc: 0.9987\n",
      "epoch:  9, loss: 0.0148, acc: 1.0000\n",
      "Valid Acc: 0.9081\n",
      "acc_val:  0.9081\n",
      "files_svd_train: ['data/flowers_svd40/daisy/15100730728_a450c5f422_n.jpg', 'data/flowers_svd40/dandelion/2449852402_45d12b9875_n.jpg']\n",
      "len(dataloader_train): 1500, len(dataloader_train): 12\n",
      "len(dataset_test): 2817, len(dataloader_test): 23\n",
      "epoch:  0, loss: 1.2556, acc: 0.4993\n",
      "epoch:  1, loss: 0.4316, acc: 0.8693\n",
      "epoch:  2, loss: 0.2394, acc: 0.9220\n",
      "epoch:  3, loss: 0.1376, acc: 0.9600\n",
      "epoch:  4, loss: 0.0785, acc: 0.9833\n",
      "epoch:  5, loss: 0.0514, acc: 0.9907\n",
      "epoch:  6, loss: 0.0322, acc: 0.9967\n",
      "epoch:  7, loss: 0.0272, acc: 0.9967\n",
      "epoch:  8, loss: 0.0177, acc: 0.9993\n",
      "epoch:  9, loss: 0.0150, acc: 0.9993\n",
      "Valid Acc: 0.9141\n",
      "acc_val:  0.9141\n",
      "len(dataloader_train): 2000, len(dataloader_train): 16\n",
      "len(dataset_test): 2317, len(dataloader_test): 19\n",
      "epoch:  0, loss: 1.0495, acc: 0.6010\n",
      "epoch:  1, loss: 0.3022, acc: 0.9080\n",
      "epoch:  2, loss: 0.1790, acc: 0.9425\n",
      "epoch:  3, loss: 0.1037, acc: 0.9750\n",
      "epoch:  4, loss: 0.0673, acc: 0.9845\n",
      "epoch:  5, loss: 0.0480, acc: 0.9915\n",
      "epoch:  6, loss: 0.0311, acc: 0.9950\n",
      "epoch:  7, loss: 0.0243, acc: 0.9975\n",
      "epoch:  8, loss: 0.0171, acc: 0.9980\n",
      "epoch:  9, loss: 0.0156, acc: 0.9975\n",
      "Valid Acc: 0.9210\n",
      "acc_val:  0.9210\n",
      "files_svd_train: ['data/flowers_svd40/daisy/14350958832_29bdd3a254.jpg', 'data/flowers_svd40/daisy/2611119198_9d46b94392.jpg']\n",
      "len(dataloader_train): 2000, len(dataloader_train): 16\n",
      "len(dataset_test): 2317, len(dataloader_test): 19\n",
      "epoch:  0, loss: 1.0780, acc: 0.5880\n",
      "epoch:  1, loss: 0.3271, acc: 0.8845\n",
      "epoch:  2, loss: 0.1824, acc: 0.9430\n",
      "epoch:  3, loss: 0.1054, acc: 0.9720\n",
      "epoch:  4, loss: 0.0727, acc: 0.9855\n",
      "epoch:  5, loss: 0.0418, acc: 0.9935\n",
      "epoch:  6, loss: 0.0278, acc: 0.9950\n",
      "epoch:  7, loss: 0.0214, acc: 0.9970\n",
      "epoch:  8, loss: 0.0162, acc: 0.9975\n",
      "epoch:  9, loss: 0.0116, acc: 0.9990\n",
      "Valid Acc: 0.9176\n",
      "acc_val:  0.9176\n",
      "len(dataloader_train): 2500, len(dataloader_train): 20\n",
      "len(dataset_test): 1817, len(dataloader_test): 15\n",
      "epoch:  0, loss: 0.9457, acc: 0.6396\n",
      "epoch:  1, loss: 0.2823, acc: 0.9016\n",
      "epoch:  2, loss: 0.1467, acc: 0.9584\n",
      "epoch:  3, loss: 0.0946, acc: 0.9752\n",
      "epoch:  4, loss: 0.0610, acc: 0.9864\n",
      "epoch:  5, loss: 0.0408, acc: 0.9940\n",
      "epoch:  6, loss: 0.0274, acc: 0.9968\n",
      "epoch:  7, loss: 0.0198, acc: 0.9980\n",
      "epoch:  8, loss: 0.0147, acc: 0.9988\n",
      "epoch:  9, loss: 0.0141, acc: 0.9988\n",
      "Valid Acc: 0.9285\n",
      "acc_val:  0.9285\n",
      "files_svd_train: ['data/flowers_svd40/tulip/20282921225_6033da8cd5_n.jpg', 'data/flowers_svd40/tulip/14255917256_84c23c572b.jpg']\n",
      "len(dataloader_train): 2500, len(dataloader_train): 20\n",
      "len(dataset_test): 1817, len(dataloader_test): 15\n",
      "epoch:  0, loss: 0.9826, acc: 0.6220\n",
      "epoch:  1, loss: 0.3076, acc: 0.8972\n",
      "epoch:  2, loss: 0.1580, acc: 0.9516\n",
      "epoch:  3, loss: 0.0980, acc: 0.9744\n",
      "epoch:  4, loss: 0.0576, acc: 0.9864\n",
      "epoch:  5, loss: 0.0384, acc: 0.9944\n",
      "epoch:  6, loss: 0.0236, acc: 0.9968\n",
      "epoch:  7, loss: 0.0189, acc: 0.9980\n",
      "epoch:  8, loss: 0.0168, acc: 0.9976\n",
      "epoch:  9, loss: 0.0143, acc: 0.9980\n",
      "Valid Acc: 0.9268\n",
      "acc_val:  0.9268\n",
      "len(dataloader_train): 3000, len(dataloader_train): 24\n",
      "len(dataset_test): 1317, len(dataloader_test): 11\n",
      "epoch:  0, loss: 0.8539, acc: 0.6700\n",
      "epoch:  1, loss: 0.2486, acc: 0.9140\n",
      "epoch:  2, loss: 0.1363, acc: 0.9577\n",
      "epoch:  3, loss: 0.0826, acc: 0.9777\n",
      "epoch:  4, loss: 0.0510, acc: 0.9903\n",
      "epoch:  5, loss: 0.0333, acc: 0.9933\n",
      "epoch:  6, loss: 0.0239, acc: 0.9973\n",
      "epoch:  7, loss: 0.0188, acc: 0.9983\n",
      "epoch:  8, loss: 0.0152, acc: 0.9983\n",
      "epoch:  9, loss: 0.0138, acc: 0.9983\n",
      "Valid Acc: 0.9347\n",
      "acc_val:  0.9347\n",
      "files_svd_train: ['data/flowers_svd40/daisy/1285423653_18926dc2c8_n.jpg', 'data/flowers_svd40/sunflower/3466923719_b4b6df7f8b_n.jpg']\n",
      "len(dataloader_train): 3000, len(dataloader_train): 24\n",
      "len(dataset_test): 1317, len(dataloader_test): 11\n",
      "epoch:  0, loss: 0.8857, acc: 0.6593\n",
      "epoch:  1, loss: 0.2708, acc: 0.9080\n",
      "epoch:  2, loss: 0.1519, acc: 0.9547\n",
      "epoch:  3, loss: 0.0876, acc: 0.9757\n",
      "epoch:  4, loss: 0.0522, acc: 0.9893\n",
      "epoch:  5, loss: 0.0315, acc: 0.9947\n",
      "epoch:  6, loss: 0.0239, acc: 0.9963\n",
      "epoch:  7, loss: 0.0200, acc: 0.9973\n",
      "epoch:  8, loss: 0.0134, acc: 0.9987\n",
      "epoch:  9, loss: 0.0102, acc: 0.9993\n",
      "Valid Acc: 0.9355\n",
      "acc_val:  0.9355\n",
      "len(dataloader_train): 3500, len(dataloader_train): 28\n",
      "len(dataset_test): 817, len(dataloader_test): 7\n",
      "epoch:  0, loss: 0.7672, acc: 0.7186\n",
      "epoch:  1, loss: 0.2395, acc: 0.9143\n",
      "epoch:  2, loss: 0.1313, acc: 0.9586\n",
      "epoch:  3, loss: 0.0883, acc: 0.9737\n",
      "epoch:  4, loss: 0.0563, acc: 0.9863\n",
      "epoch:  5, loss: 0.0305, acc: 0.9946\n",
      "epoch:  6, loss: 0.0226, acc: 0.9974\n",
      "epoch:  7, loss: 0.0163, acc: 0.9986\n",
      "epoch:  8, loss: 0.0125, acc: 0.9986\n",
      "epoch:  9, loss: 0.0116, acc: 0.9991\n",
      "Valid Acc: 0.9474\n",
      "acc_val:  0.9474\n",
      "files_svd_train: ['data/flowers_svd40/tulip/485266837_671def8627.jpg', 'data/flowers_svd40/tulip/13979840624_28466cb3ec_n.jpg']\n",
      "len(dataloader_train): 3500, len(dataloader_train): 28\n",
      "len(dataset_test): 817, len(dataloader_test): 7\n",
      "epoch:  0, loss: 0.8026, acc: 0.7066\n",
      "epoch:  1, loss: 0.2655, acc: 0.9091\n",
      "epoch:  2, loss: 0.1446, acc: 0.9546\n",
      "epoch:  3, loss: 0.0875, acc: 0.9737\n",
      "epoch:  4, loss: 0.0530, acc: 0.9874\n",
      "epoch:  5, loss: 0.0293, acc: 0.9957\n",
      "epoch:  6, loss: 0.0182, acc: 0.9983\n",
      "epoch:  7, loss: 0.0153, acc: 0.9974\n",
      "epoch:  8, loss: 0.0118, acc: 0.9983\n",
      "epoch:  9, loss: 0.0100, acc: 0.9986\n",
      "Valid Acc: 0.9523\n",
      "acc_val:  0.9523\n",
      "results_flip:  [tensor(0.8790, device='cuda:1', dtype=torch.float64), tensor(0.8993, device='cuda:1', dtype=torch.float64), tensor(0.9081, device='cuda:1', dtype=torch.float64), tensor(0.9210, device='cuda:1', dtype=torch.float64), tensor(0.9285, device='cuda:1', dtype=torch.float64), tensor(0.9347, device='cuda:1', dtype=torch.float64), tensor(0.9474, device='cuda:1', dtype=torch.float64)]\n",
      "results_svd_flip:  [tensor(0.8805, device='cuda:1', dtype=torch.float64), tensor(0.9011, device='cuda:1', dtype=torch.float64), tensor(0.9141, device='cuda:1', dtype=torch.float64), tensor(0.9176, device='cuda:1', dtype=torch.float64), tensor(0.9268, device='cuda:1', dtype=torch.float64), tensor(0.9355, device='cuda:1', dtype=torch.float64), tensor(0.9523, device='cuda:1', dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "results_flip = []\n",
    "results_svd_flip =[]\n",
    "for size in [500, 1000, 1500, 2000, 2500, 3000, 3500]:\n",
    "    acc_flip = train_eval(transform_compose_flip, train_size=size, train_epochs=10)\n",
    "    acc_svd_flip = train_eval_svd(transform_compose, train_size=size, train_epochs=10)\n",
    "    results_flip.append(acc_flip)\n",
    "    results_svd_flip.append(acc_svd_flip)\n",
    "print('results_flip: ', results_flip)\n",
    "print('results_svd_flip: ', results_svd_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0418750b-99c1-4705-9397-ffba9c777b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.8790, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.8993, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.9081, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.9210, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.9285, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.9347, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.9474, device='cuda:1', dtype=torch.float64)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ddc92a0-0ccc-4368-94f1-9741097779f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.8805, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.9011, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.9141, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.9176, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.9268, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.9355, device='cuda:1', dtype=torch.float64),\n",
       " tensor(0.9523, device='cuda:1', dtype=torch.float64)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_svd_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "812736e1-e112-4645-9b6b-4acee97eff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = []\n",
    "results_all.append(np.array([x.cpu().item() for x in results]))\n",
    "results_all.append(np.array([x.cpu().item() for x in results_svd]))\n",
    "results_all.append(np.array([x.cpu().item() for x in results_flip]))\n",
    "results_all.append(np.array([x.cpu().item() for x in results_svd_flip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa46cc34-4ed2-4e92-a227-b4b209737a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/results_classification.npy', np.array(results_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "81e9cd94-6c29-4d8f-885b-9719c2fb6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.4f\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6cee06e-eefd-44f9-a727-afa48f416cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.8779, 0.8939, 0.9049, 0.9145, 0.9263, 0.9286, 0.9461]),\n",
       " array([0.8800, 0.9011, 0.9134, 0.9176, 0.9268, 0.9355, 0.9510]),\n",
       " array([0.8790, 0.8993, 0.9081, 0.9210, 0.9285, 0.9347, 0.9474]),\n",
       " array([0.8805, 0.9011, 0.9141, 0.9176, 0.9268, 0.9355, 0.9523])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
